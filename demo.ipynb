{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Sinergym\n",
    "import sinergym\n",
    "from sinergym.utils.rewards import ExpReward\n",
    "from src.sinergym_wrapper import SinergymWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the EnergyPlus path to the system path\n",
    "sys.path.append('./EnergyPlus-23-1-0')\n",
    "# Set the EnergyPlus path as an environment variable\n",
    "os.environ['EPLUS_PATH'] = './EnergyPlus-23-1-0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment ID\n",
    "id = \"Eplus-5zone-hot-continuous-stochastic-v1\"\n",
    "\n",
    "# Weather\n",
    "weather_files = ['USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw']\n",
    "\n",
    "# mu, sigma and theta for the weather variability\n",
    "# In the original version, weather_variability was a triple (mu, sigma, theta)\n",
    "# that only affected the drybulb (outdoor temperature), while now it is a dictionary\n",
    "# with the mean, sigma and theta for each weather variable we want to vary\n",
    "# NOTE: Check apply_weather_variability method in CustomModelJSON class\n",
    "weather_variability = {\n",
    "    'drybulb': np.array([5.53173187e+00, 0.00000000e+00, 2.55034944e-03]), \n",
    "    'relhum': np.array([1.73128872e+01, 0.00000000e+00, 2.31712760e-03]), \n",
    "    'winddir': np.array([7.39984654e+01, 0.00000000e+00, 4.02298013e-04]), \n",
    "    'dirnorrad': np.array([3.39506556e+02, 0.00000000e+00, 9.78192172e-04]), \n",
    "    'windspd': np.array([1.64655725e+00, 0.00000000e+00, 3.45045547e-04])}\n",
    "\n",
    "# Reward function and kwargs\n",
    "reward = ExpReward\n",
    "reward_kwargs = {\n",
    "    'temperature_variables': ['air_temperature'],\n",
    "    'energy_variables': ['HVAC_electricity_demand_rate'],\n",
    "    'range_comfort_winter': [20, 23],\n",
    "    'range_comfort_summer': [23, 26],\n",
    "    'energy_weight': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#==============================================================================================#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Creating Gymnasium environment... [5zone-hot-continuous-stochastic-v1]\u001b[0m\n",
      "#==============================================================================================#\n",
      "\u001b[38;20m[MODELING] (INFO) : Experiment working directory created [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-5zone-hot-continuous-stochastic-v1-res1]\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Model Config is correct.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Variable available names\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Meter available names\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : runperiod established: {'start_day': 1, 'start_month': 1, 'start_year': 1991, 'end_day': 31, 'end_month': 12, 'end_year': 1991, 'start_weekday': 0, 'n_steps_per_hour': 4}\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode length (seconds): 31536000.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timestep size (seconds): 900.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timesteps per episode: 35040\u001b[0m\n",
      "\u001b[38;20m[REWARD] (INFO) : Reward function initialized.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment 5zone-hot-continuous-stochastic-v1 created successfully.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Experiment working directory created [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1]\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Model Config is correct.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Variable available names\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Meter available names\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : runperiod established: {'start_day': 1, 'start_month': 1, 'start_year': 1991, 'end_day': 31, 'end_month': 12, 'end_year': 1991, 'start_weekday': 0, 'n_steps_per_hour': 4}\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode length (seconds): 31536000.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timestep size (seconds): 900.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timesteps per episode: 35040\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\n",
    "    id=id,\n",
    "    weather_files=weather_files,\n",
    "    reward=reward,\n",
    "    reward_kwargs=reward_kwargs,\n",
    "    weather_variability=weather_variability\n",
    ")\n",
    "\n",
    "# Wrap the environment (to account for the different weather variability)\n",
    "env =  SinergymWrapper(env)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    #==================================================================================#\n",
      "                                ENVIRONMENT NAME: 5zone-hot-continuous-stochastic-v1\n",
      "    #==================================================================================#\n",
      "    #----------------------------------------------------------------------------------#\n",
      "                                ENVIRONMENT INFO:\n",
      "    #----------------------------------------------------------------------------------#\n",
      "    - Building file: /home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/sinergym/data/buildings/5ZoneAutoDXVAV.epJSON\n",
      "    - Zone names: ['PLENUM-1', 'SPACE1-1', 'SPACE2-1', 'SPACE3-1', 'SPACE4-1', 'SPACE5-1']\n",
      "    - Weather file(s): ['USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw']\n",
      "    - Current weather used: /home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/sinergym/data/weather/USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw\n",
      "    - Episodes executed: 0\n",
      "    - Workspace directory: /home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1\n",
      "    - Reward function: <sinergym.utils.rewards.ExpReward object at 0x7f2b0acac1c0>\n",
      "    - Reset default options: {'weather_variability': {'drybulb': array([5.53173187e+00, 0.00000000e+00, 2.55034944e-03]), 'relhum': array([1.73128872e+01, 0.00000000e+00, 2.31712760e-03]), 'winddir': array([7.39984654e+01, 0.00000000e+00, 4.02298013e-04]), 'dirnorrad': array([3.39506556e+02, 0.00000000e+00, 9.78192172e-04]), 'windspd': array([1.64655725e+00, 0.00000000e+00, 3.45045547e-04])}}\n",
      "    - Run period: {'start_day': 1, 'start_month': 1, 'start_year': 1991, 'end_day': 31, 'end_month': 12, 'end_year': 1991, 'start_weekday': 0, 'n_steps_per_hour': 4}\n",
      "    - Episode length: 31536000.0\n",
      "    - Number of timesteps in an episode: 35040\n",
      "    - Timestep size (seconds): 900.0\n",
      "    - It is discrete?: False\n",
      "    #----------------------------------------------------------------------------------#\n",
      "                                ENVIRONMENT SPACE:\n",
      "    #----------------------------------------------------------------------------------#\n",
      "    - Observation space: Box(-50000000.0, 50000000.0, (17,), float32)\n",
      "    - Observation variables: ['month', 'day_of_month', 'hour', 'outdoor_temperature', 'outdoor_humidity', 'wind_speed', 'wind_direction', 'diffuse_solar_radiation', 'direct_solar_radiation', 'htg_setpoint', 'clg_setpoint', 'air_temperature', 'air_humidity', 'people_occupant', 'co2_emission', 'HVAC_electricity_demand_rate', 'total_electricity_HVAC']\n",
      "    - Action space: Box([12.  23.5], [21.5 40. ], (2,), float32)\n",
      "    - Action variables: ['Heating_Setpoint_RL', 'Cooling_Setpoint_RL']\n",
      "    #==================================================================================#\n",
      "                                    SIMULATOR\n",
      "    #==================================================================================#\n",
      "    *NOTE: To have information about available handlers and controlled elements, it is\n",
      "    required to do env reset before to print information.*\n",
      "\n",
      "    Is running? : False\n",
      "    #----------------------------------------------------------------------------------#\n",
      "                                AVAILABLE ELEMENTS:\n",
      "    #----------------------------------------------------------------------------------#\n",
      "    *Some variables can not be here depending if it is defined Output:Variable field\n",
      "     in building model. See documentation for more information.*\n",
      "\n",
      "    None\n",
      "    #----------------------------------------------------------------------------------#\n",
      "                                CONTROLLED ELEMENTS:\n",
      "    #----------------------------------------------------------------------------------#\n",
      "    - Actuators: None\n",
      "    - Variables: None\n",
      "    - Meters: None\n",
      "    - Internal Variables: None\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "env.unwrapped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Starting a new episode... [5zone-hot-continuous-stochastic-v1] [Episode 1]\u001b[0m\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode directory created [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1]\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Weather file USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw used.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Adapting weather to building model. [USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw]\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Saving episode output path... [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1/output]\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : Running EnergyPlus with args: ['-w', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1/USA_AZ_Davis-Monthan.AFB.722745_TMY3_Randomized.epw', '-d', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1/output', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1/5ZoneAutoDXVAV.epJSON']\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 1 started.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers initialized.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers are ready.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : System is ready.\u001b[0m\n",
      "\n",
      "\n",
      "OBSERVATION:  [1.0000000e+00 1.0000000e+00 0.0000000e+00 9.0797100e+00 5.8616100e+01\n",
      " 3.9327860e+00 1.2714914e+02 0.0000000e+00 0.0000000e+00 1.4517688e+01\n",
      " 3.1385578e+01 1.9513937e+01 4.2447315e+01 0.0000000e+00 0.0000000e+00\n",
      " 4.7434094e+02 4.2690684e+05]\n",
      "REWARD:  -48.89740559615044\n",
      "TERMINATED:  False\n",
      "TRUNCATED:  False\n",
      "INFO:  {'time_elapsed(hours)': 0.5, 'month': 1, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([14.517688, 31.385578], dtype=float32), 'timestep': 2, 'reward': -48.89740559615044, 'energy_term': -47.434092869631144, 'comfort_term': -1.4633127265192982, 'reward_weight': 0.1, 'abs_energy': 474.3409286963114, 'abs_comfort': 1.625903029465887, 'energy_values': [474.3409286963114], 'temp_values': [19.51393662812762]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    obs, info = env.reset()\n",
    "    rewards = []\n",
    "    terminated = False\n",
    "    current_month = 0\n",
    "    while not terminated:\n",
    "        a = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        if info['month'] != current_month:  # display results every month\n",
    "            current_month = info['month']\n",
    "            print(\"\\n\\nOBSERVATION: \", obs)\n",
    "            print(\"REWARD: \", reward)\n",
    "            print(\"TERMINATED: \", terminated)\n",
    "            print(\"TRUNCATED: \", truncated)\n",
    "            print(\"INFO: \", info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 18:24:55.437214: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-06 18:24:55.467703: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-06 18:24:55.467729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-06 18:24:55.468654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-06 18:24:55.474625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-06 18:24:56.092949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Starting a new episode... [5zone-hot-continuous-stochastic-v1] [Episode 2]\u001b[0m\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode directory created [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2]\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Weather file USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw used.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Adapting weather to building model. [USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw]\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Saving episode output path... [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2/output]\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : Running EnergyPlus with args: ['-w', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2/USA_AZ_Davis-Monthan.AFB.722745_TMY3_Randomized.epw', '-d', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2/output', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2/5ZoneAutoDXVAV.epJSON']\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 2 started.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers are ready.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : System is ready.\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------| 6%\n",
      "| time/              |      |\n",
      "|    fps             | 424  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "--------------------------------------------------------------------------------------------------------------| 12%\n",
      "| time/                   |              |\n",
      "|    fps                  | 572          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030761333 |\n",
      "|    clip_fraction        | 0.00249      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.31e+07     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000621    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.21e+07     |\n",
      "------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------| 18%\n",
      "| time/                   |              |\n",
      "|    fps                  | 670          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018937367 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.18e+06     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000239    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.58e+07     |\n",
      "------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------| 23%\n",
      "| time/                   |              |\n",
      "|    fps                  | 740          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063194325 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.3e+06      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00386     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 9.73e+06     |\n",
      "------------------------------------------\n",
      "--------------------------------------------------------------------------------------------------------------| 29%\n",
      "| time/                   |              |\n",
      "|    fps                  | 786          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032574167 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.98e+06     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.54e+07     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |***************************************************************************************************| 99%\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the PPO agent\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the agent\n",
    "model.save(\"ppo_agent\")\n",
    "\n",
    "# To load the trained agent\n",
    "# model = PPO.load(\"ppo_agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |***************************************************************************************************| 99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment closed. [5zone-hot-continuous-stochastic-v1]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# A constant to use for any configuration that should be deprecated\n",
    "# (to check, whether this config has actually been assigned a proper value or\n",
    "# not).\n",
    "DEPRECATED_VALUE = -1\n",
    "\n",
    "model_config = {\n",
    "    # Experimental flag.\n",
    "    # If True, try to use a native (tf.keras.Model or torch.Module) default\n",
    "    # model instead of our built-in ModelV2 defaults.\n",
    "    # If False (default), use \"classic\" ModelV2 default models.\n",
    "    # Note that this currently only works for:\n",
    "    # 1) framework != torch AND\n",
    "    # 2) fully connected and CNN default networks as well as\n",
    "    # auto-wrapped LSTM- and attention nets.\n",
    "    \"_use_default_native_models\": False,\n",
    "    # Experimental flag.\n",
    "    # If True, user specified no preprocessor to be created\n",
    "    # (via config._disable_preprocessor_api=True). If True, observations\n",
    "    # will arrive in model as they are returned by the env.\n",
    "    \"_disable_preprocessor_api\": False,\n",
    "    # Experimental flag.\n",
    "    # If True, RLlib will no longer flatten the policy-computed actions into\n",
    "    # a single tensor (for storage in SampleCollectors/output files/etc..),\n",
    "    # but leave (possibly nested) actions as-is. Disabling flattening affects:\n",
    "    # - SampleCollectors: Have to store possibly nested action structs.\n",
    "    # - Models that have the previous action(s) as part of their input.\n",
    "    # - Algorithms reading from offline files (incl. action information).\n",
    "    \"_disable_action_flattening\": False,\n",
    "\n",
    "    # === Built-in options ===\n",
    "    # FullyConnectedNetwork (tf and torch): rllib.models.tf|torch.fcnet.py\n",
    "    # These are used if no custom model is specified and the input space is 1D.\n",
    "    # Number of hidden layers to be used.\n",
    "    \"fcnet_hiddens\": [256, 256],\n",
    "    # Activation function descriptor.\n",
    "    # Supported values are: \"tanh\", \"relu\", \"swish\" (or \"silu\"),\n",
    "    # \"linear\" (or None).\n",
    "    \"fcnet_activation\": \"tanh\",\n",
    "\n",
    "    # VisionNetwork (tf and torch): rllib.models.tf|torch.visionnet.py\n",
    "    # These are used if no custom model is specified and the input space is 2D.\n",
    "    # Filter config: List of [out_channels, kernel, stride] for each filter.\n",
    "    # Example:\n",
    "    # Use None for making RLlib try to find a default filter setup given the\n",
    "    # observation space.\n",
    "    \"conv_filters\": None,\n",
    "    # Activation function descriptor.\n",
    "    # Supported values are: \"tanh\", \"relu\", \"swish\" (or \"silu\"),\n",
    "    # \"linear\" (or None).\n",
    "    \"conv_activation\": \"relu\",\n",
    "\n",
    "    # Some default models support a final FC stack of n Dense layers with given\n",
    "    # activation:\n",
    "    # - Complex observation spaces: Image components are fed through\n",
    "    #   VisionNets, flat Boxes are left as-is, Discrete are one-hot'd, then\n",
    "    #   everything is concated and pushed through this final FC stack.\n",
    "    # - VisionNets (CNNs), e.g. after the CNN stack, there may be\n",
    "    #   additional Dense layers.\n",
    "    # - FullyConnectedNetworks will have this additional FCStack as well\n",
    "    # (that's why it's empty by default).\n",
    "    \"post_fcnet_hiddens\": [],\n",
    "    \"post_fcnet_activation\": \"relu\",\n",
    "\n",
    "    # For DiagGaussian action distributions, make the second half of the model\n",
    "    # outputs floating bias variables instead of state-dependent. This only\n",
    "    # has an effect is using the default fully connected net.\n",
    "    \"free_log_std\": False,\n",
    "    # Whether to skip the final linear layer used to resize the hidden layer\n",
    "    # outputs to size `num_outputs`. If True, then the last hidden layer\n",
    "    # should already match num_outputs.\n",
    "    \"no_final_linear\": False,\n",
    "    # Whether layers should be shared for the value function.\n",
    "    \"vf_share_layers\": True,\n",
    "\n",
    "    # == LSTM ==\n",
    "    # Whether to wrap the model with an LSTM.\n",
    "    \"use_lstm\": False,\n",
    "    # Max seq len for training the LSTM, defaults to 20.\n",
    "    \"max_seq_len\": 20,\n",
    "    # Size of the LSTM cell.\n",
    "    \"lstm_cell_size\": 256,\n",
    "    # Whether to feed a_{t-1} to LSTM (one-hot encoded if discrete).\n",
    "    \"lstm_use_prev_action\": False,\n",
    "    # Whether to feed r_{t-1} to LSTM.\n",
    "    \"lstm_use_prev_reward\": False,\n",
    "    # Whether the LSTM is time-major (TxBx..) or batch-major (BxTx..).\n",
    "    \"_time_major\": False,\n",
    "\n",
    "    # == Attention Nets (experimental: torch-version is untested) ==\n",
    "    # Whether to use a GTrXL (\"Gru transformer XL\"; attention net) as the\n",
    "    # wrapper Model around the default Model.\n",
    "    \"use_attention\": False,\n",
    "    # The number of transformer units within GTrXL.\n",
    "    # A transformer unit in GTrXL consists of a) MultiHeadAttention module and\n",
    "    # b) a position-wise MLP.\n",
    "    \"attention_num_transformer_units\": 1,\n",
    "    # The input and output size of each transformer unit.\n",
    "    \"attention_dim\": 64,\n",
    "    # The number of attention heads within the MultiHeadAttention units.\n",
    "    \"attention_num_heads\": 1,\n",
    "    # The dim of a single head (within the MultiHeadAttention units).\n",
    "    \"attention_head_dim\": 32,\n",
    "    # The memory sizes for inference and training.\n",
    "    \"attention_memory_inference\": 50,\n",
    "    \"attention_memory_training\": 50,\n",
    "    # The output dim of the position-wise MLP.\n",
    "    \"attention_position_wise_mlp_dim\": 32,\n",
    "    # The initial bias values for the 2 GRU gates within a transformer unit.\n",
    "    \"attention_init_gru_gate_bias\": 2.0,\n",
    "    # Whether to feed a_{t-n:t-1} to GTrXL (one-hot encoded if discrete).\n",
    "    \"attention_use_n_prev_actions\": 0,\n",
    "    # Whether to feed r_{t-n:t-1} to GTrXL.\n",
    "    \"attention_use_n_prev_rewards\": 0,\n",
    "\n",
    "    # == Atari ==\n",
    "    # Set to True to enable 4x stacking behavior.\n",
    "    \"framestack\": True,\n",
    "    # Final resized frame dimension\n",
    "    \"dim\": 84,\n",
    "    # (deprecated) Converts ATARI frame to 1 Channel Grayscale image\n",
    "    \"grayscale\": False,\n",
    "    # (deprecated) Changes frame to range from [-1, 1] if true\n",
    "    \"zero_mean\": True,\n",
    "\n",
    "    # === Options for custom models ===\n",
    "    # Name of a custom model to use\n",
    "    \"custom_model\": None,\n",
    "    # Extra options to pass to the custom classes. These will be available to\n",
    "    # the Model's constructor in the model_config field. Also, they will be\n",
    "    # attempted to be passed as **kwargs to ModelV2 models. For an example,\n",
    "    # see rllib/models/[tf|torch]/attention_net.py.\n",
    "    \"custom_model_config\": {},\n",
    "    # Name of a custom action distribution to use.\n",
    "    \"custom_action_dist\": None,\n",
    "    # Custom preprocessors are deprecated. Please use a wrapper class around\n",
    "    # your environment instead to preprocess observations.\n",
    "    \"custom_preprocessor\": None,\n",
    "\n",
    "    # Deprecated keys:\n",
    "    # Use `lstm_use_prev_action` or `lstm_use_prev_reward` instead.\n",
    "    \"lstm_use_prev_action_reward\": DEPRECATED_VALUE,\n",
    "    \n",
    "\n",
    "    \"fcnet_activation\": lambda: nn.Sequential(nn.Tanh(), nn.Dropout(p=0.1)),\n",
    "    \"num_dropout_evals\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
      "2024-02-06 18:25:15,431\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "2024-02-06 18:25:19,752\tERROR actor_manager.py:486 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=29025, ip=192.168.1.40, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbef53a4f70>)\n",
      "  File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 662, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "  File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2182, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "2024-02-06 18:25:19,753\tERROR actor_manager.py:486 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=29026, ip=192.168.1.40, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd9ac09ceb0>)\n",
      "  File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 662, in __init__\n",
      "    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "  File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2182, in get_multi_agent_setup\n",
      "    raise ValueError(\n",
      "ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29026)\u001b[0m 2024-02-06 18:25:19,750\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=29026, ip=192.168.1.40, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fd9ac09ceb0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29026)\u001b[0m   File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 662, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29026)\u001b[0m     self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29026)\u001b[0m   File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2182, in get_multi_agent_setup\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29026)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29026)\u001b[0m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29025)\u001b[0m 2024-02-06 18:25:19,742\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=29025, ip=192.168.1.40, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbef53a4f70>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29025)\u001b[0m   File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 662, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29025)\u001b[0m     self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29025)\u001b[0m   File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2182, in get_multi_agent_setup\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29025)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29025)\u001b[0m ValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:169\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:239\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_workers_after_construction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:612\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget()\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py:473\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, timeout_seconds, return_obj_refs)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     remote_results\u001b[38;5;241m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[38;5;241m=\u001b[39mresult))\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/_private/worker.py:2311\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2311\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=29025, ip=192.168.1.40, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fbef53a4f70>)\n  File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 662, in __init__\n    self.policy_dict, self.is_policy_to_train = self.config.get_multi_agent_setup(\n  File \"/home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py\", line 2182, in get_multi_agent_setup\n    raise ValueError(\nValueError: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muncertain_ppo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UncertainPPO\n\u001b[0;32m----> 3\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mUncertainPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:441\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m     }\n\u001b[1;32m    439\u001b[0m }\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:169\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    167\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:566\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;66;03m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;66;03m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;66;03m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:191\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# errors.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mactor_init_failed:\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mValueError\u001b[0m: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!"
     ]
    }
   ],
   "source": [
    "from src.uncertain_ppo import UncertainPPO\n",
    "\n",
    "agent = UncertainPPO(config={'model': model_config})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
