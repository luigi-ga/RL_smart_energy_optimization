{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Sinergym\n",
    "import sinergym\n",
    "from src.sinergym_wrapper import SinergymWrapper\n",
    "from src.custom_reward import FangerReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the EnergyPlus path to the system path\n",
    "sys.path.append('./EnergyPlus-23-1-0')\n",
    "# Set the EnergyPlus path as an environment variable\n",
    "os.environ['EPLUS_PATH'] = './EnergyPlus-23-1-0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment ID\n",
    "id = \"Eplus-5zone-hot-continuous-stochastic-v1\"\n",
    "\n",
    "# Weather\n",
    "weather_files = ['USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw']\n",
    "\n",
    "# mu, sigma and theta for the weather variability\n",
    "# In the original version, weather_variability was a triple (mu, sigma, theta)\n",
    "# that only affected the drybulb (outdoor temperature), while now it is a dictionary\n",
    "# with the mean, sigma and theta for each weather variable we want to vary.\n",
    "# NOTE: Check apply_weather_variability method in CustomModelJSON class\n",
    "weather_variability = {\n",
    "    'drybulb': np.array([5.53173187e+00, 0.00000000e+00, 2.55034944e-03]), \n",
    "    'relhum': np.array([1.73128872e+01, 0.00000000e+00, 2.31712760e-03]), \n",
    "    'winddir': np.array([7.39984654e+01, 0.00000000e+00, 4.02298013e-04]), \n",
    "    'dirnorrad': np.array([3.39506556e+02, 0.00000000e+00, 9.78192172e-04]), \n",
    "    'windspd': np.array([1.64655725e+00, 0.00000000e+00, 3.45045547e-04])}\n",
    "\n",
    "# Custom reward derived from Fanger's comfort model.\n",
    "# This extends the LinearReward class from sinergym adding ppd and occupancy variables.\n",
    "reward = FangerReward\n",
    "reward_kwargs={\n",
    "    'temperature_variables': ['air_temperature'],\n",
    "    'energy_variables': ['HVAC_electricity_demand_rate'],\n",
    "    'range_comfort_winter': [20, 23],\n",
    "    'range_comfort_summer': [23, 26],\n",
    "    'energy_weight': 0.1,\n",
    "    'ppd_variable': 'Zone Thermal Comfort Fanger Model PPD(SPACE1-1 PEOPLE 1)',\n",
    "    'occupancy_variable': 'Zone People Occupant Count(SPACE1-1)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#==============================================================================================#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Creating Gymnasium environment... [5zone-hot-continuous-stochastic-v1]\u001b[0m\n",
      "#==============================================================================================#\n",
      "\u001b[38;20m[MODELING] (INFO) : Experiment working directory created [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-5zone-hot-continuous-stochastic-v1-res1]\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Model Config is correct.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Variable available names\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Meter available names\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : runperiod established: {'start_day': 1, 'start_month': 1, 'start_year': 1991, 'end_day': 31, 'end_month': 12, 'end_year': 1991, 'start_weekday': 0, 'n_steps_per_hour': 4}\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode length (seconds): 31536000.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timestep size (seconds): 900.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timesteps per episode: 35040\u001b[0m\n",
      "\u001b[38;20m[REWARD] (INFO) : Reward function initialized.\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment 5zone-hot-continuous-stochastic-v1 created successfully.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Experiment working directory created [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1]\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Model Config is correct.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Variable available names\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Updated building model with whole Output:Meter available names\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : runperiod established: {'start_day': 1, 'start_month': 1, 'start_year': 1991, 'end_day': 31, 'end_month': 12, 'end_year': 1991, 'start_weekday': 0, 'n_steps_per_hour': 4}\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode length (seconds): 31536000.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timestep size (seconds): 900.0\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : timesteps per episode: 35040\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\n",
    "    id=id,\n",
    "    weather_files=weather_files,\n",
    "    reward=reward,\n",
    "    reward_kwargs=reward_kwargs,\n",
    "    weather_variability=weather_variability\n",
    ")\n",
    "\n",
    "# Wrap the environment (to account for the different weather variability)\n",
    "env =  SinergymWrapper(env)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    #==================================================================================#\n",
      "                                ENVIRONMENT NAME: 5zone-hot-continuous-stochastic-v1\n",
      "    #==================================================================================#\n",
      "    #----------------------------------------------------------------------------------#\n",
      "                                ENVIRONMENT INFO:\n",
      "    #----------------------------------------------------------------------------------#\n",
      "    - Building file: /home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/sinergym/data/buildings/5ZoneAutoDXVAV.epJSON\n",
      "    - Zone names: ['PLENUM-1', 'SPACE1-1', 'SPACE2-1', 'SPACE3-1', 'SPACE4-1', 'SPACE5-1']\n",
      "    - Weather file(s): ['USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw']\n",
      "    - Current weather used: /home/luigi/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/sinergym/data/weather/USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw\n",
      "    - Episodes executed: 0\n",
      "    - Workspace directory: /home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1\n",
      "    - Reward function: <src.custom_reward.FangerReward object at 0x7fe95f09c070>\n",
      "    - Reset default options: {'weather_variability': {'drybulb': array([5.53173187e+00, 0.00000000e+00, 2.55034944e-03]), 'relhum': array([1.73128872e+01, 0.00000000e+00, 2.31712760e-03]), 'winddir': array([7.39984654e+01, 0.00000000e+00, 4.02298013e-04]), 'dirnorrad': array([3.39506556e+02, 0.00000000e+00, 9.78192172e-04]), 'windspd': array([1.64655725e+00, 0.00000000e+00, 3.45045547e-04])}}\n",
      "    - Run period: {'start_day': 1, 'start_month': 1, 'start_year': 1991, 'end_day': 31, 'end_month': 12, 'end_year': 1991, 'start_weekday': 0, 'n_steps_per_hour': 4}\n",
      "    - Episode length: 31536000.0\n",
      "    - Number of timesteps in an episode: 35040\n",
      "    - Timestep size (seconds): 900.0\n",
      "    - It is discrete?: False\n",
      "    #----------------------------------------------------------------------------------#\n",
      "                                ENVIRONMENT SPACE:\n",
      "    #----------------------------------------------------------------------------------#\n",
      "    - Observation space: Box(-50000000.0, 50000000.0, (17,), float32)\n",
      "    - Observation variables: ['month', 'day_of_month', 'hour', 'outdoor_temperature', 'outdoor_humidity', 'wind_speed', 'wind_direction', 'diffuse_solar_radiation', 'direct_solar_radiation', 'htg_setpoint', 'clg_setpoint', 'air_temperature', 'air_humidity', 'people_occupant', 'co2_emission', 'HVAC_electricity_demand_rate', 'total_electricity_HVAC']\n",
      "    - Action space: Box([12.  23.5], [21.5 40. ], (2,), float32)\n",
      "    - Action variables: ['Heating_Setpoint_RL', 'Cooling_Setpoint_RL']\n",
      "    #==================================================================================#\n",
      "                                    SIMULATOR\n",
      "    #==================================================================================#\n",
      "    *NOTE: To have information about available handlers and controlled elements, it is\n",
      "    required to do env reset before to print information.*\n",
      "\n",
      "    Is running? : False\n",
      "    #----------------------------------------------------------------------------------#\n",
      "                                AVAILABLE ELEMENTS:\n",
      "    #----------------------------------------------------------------------------------#\n",
      "    *Some variables can not be here depending if it is defined Output:Variable field\n",
      "     in building model. See documentation for more information.*\n",
      "\n",
      "    None\n",
      "    #----------------------------------------------------------------------------------#\n",
      "                                CONTROLLED ELEMENTS:\n",
      "    #----------------------------------------------------------------------------------#\n",
      "    - Actuators: None\n",
      "    - Variables: None\n",
      "    - Meters: None\n",
      "    - Internal Variables: None\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "env.unwrapped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Starting a new episode... [5zone-hot-continuous-stochastic-v1] [Episode 1]\u001b[0m\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode directory created [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1]\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Weather file USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw used.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Adapting weather to building model. [USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw]\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Saving episode output path... [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1/output]\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : Running EnergyPlus with args: ['-w', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1/USA_AZ_Davis-Monthan.AFB.722745_TMY3_Random_1.64655725_0.0_0.000345045547.epw', '-d', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1/output', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run1/5ZoneAutoDXVAV.epJSON']\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 1 started.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers initialized.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers are ready.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : System is ready.\u001b[0m\n",
      "\n",
      "\n",
      "OBSERVATION:  [1.0000000e+00 1.0000000e+00 0.0000000e+00 1.4460995e+00 5.1607044e+01\n",
      " 4.2139239e+00 1.5823387e+02 0.0000000e+00 0.0000000e+00 1.2800000e+01\n",
      " 4.0000000e+01 1.8302999e+01 1.5107014e+01 0.0000000e+00 0.0000000e+00\n",
      " 1.7868138e+03 1.6081325e+06]\n",
      "REWARD:  -0.01786813870879874\n",
      "TERMINATED:  False\n",
      "TRUNCATED:  False\n",
      "INFO:  {'time_elapsed(hours)': 0.5, 'month': 1, 'day': 1, 'hour': 0, 'is_raining': False, 'action': array([19.651293, 24.801943], dtype=float32), 'timestep': 2, 'reward': -0.01786813870879874, 'energy_term': -0.01786813870879874, 'comfort_term': -0.0, 'reward_weight': 0.1, 'abs_energy': 1786.8138708798738, 'abs_comfort': 0.0, 'energy_values': [1786.8138708798738], 'temp_values': [18.303000015945685]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    obs, info = env.reset()\n",
    "    rewards = []\n",
    "    terminated = False\n",
    "    current_month = 0\n",
    "    while not terminated:\n",
    "        a = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        if info['month'] != current_month:  # display results every month\n",
    "            current_month = info['month']\n",
    "            print(\"\\n\\nOBSERVATION: \", obs)\n",
    "            print(\"REWARD: \", reward)\n",
    "            print(\"TERMINATED: \", terminated)\n",
    "            print(\"TRUNCATED: \", truncated)\n",
    "            print(\"INFO: \", info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.custom_reward.FangerReward at 0x7fe95f09c070>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.reward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 15:31:53.513183: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-08 15:31:53.541342: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-08 15:31:53.541372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-08 15:31:53.542114: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-08 15:31:53.547047: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-08 15:31:54.053983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Starting a new episode... [5zone-hot-continuous-stochastic-v1] [Episode 2]\u001b[0m\n",
      "#----------------------------------------------------------------------------------------------#\n",
      "\u001b[38;20m[MODELING] (INFO) : Episode directory created [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2]\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Weather file USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw used.\u001b[0m\n",
      "\u001b[38;20m[MODELING] (INFO) : Adapting weather to building model. [USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw]\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Saving episode output path... [/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2/output]\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : Running EnergyPlus with args: ['-w', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2/USA_AZ_Davis-Monthan.AFB.722745_TMY3_Random_1.64655725_0.0_0.000345045547.epw', '-d', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2/output', '/home/luigi/Documents/SmartEnergyOptimizationRL/Eplus-env-eplus-env-v1-res1/Eplus-env-sub_run2/5ZoneAutoDXVAV.epJSON']\u001b[0m\n",
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Episode 2 started.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : handlers are ready.\u001b[0m\n",
      "\u001b[38;20m[SIMULATOR] (INFO) : System is ready.\u001b[0m\n",
      "--------------------------------------------------------------------------------------------------------------| 6%\n",
      "| time/              |      |\n",
      "|    fps             | 424  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "--------------------------------------------------------------------------------------------------------------| 12%\n",
      "| time/                   |             |\n",
      "|    fps                  | 566         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005191358 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.0398     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.054       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00344    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.079       |\n",
      "-----------------------------------------\n",
      "Progress: |*************--------------------------------------------------------------------------------------| 13%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |***************************************************************************************************| 99%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Save the agent\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:192\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    188\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39munscale_action(clipped_actions)\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhigh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(clipped_actions)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n",
      "File \u001b[0;32m~/Documents/SmartEnergyOptimizationRL/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2100\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_dispatcher\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, a_min, a_max)\n\u001b[0;32m-> 2100\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \n\u001b[1;32m   2168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create the PPO agent\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Save the agent\n",
    "model.save(\"ppo_agent\")\n",
    "\n",
    "# To load the trained agent\n",
    "# model = PPO.load(\"ppo_agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |***************************************************************************************************| 99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m[ENVIRONMENT] (INFO) : Environment closed. [5zone-hot-continuous-stochastic-v1]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
