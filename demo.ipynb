{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "# Sinergym\n",
    "import sinergym\n",
    "from src.wrappers import SinergymWrapper\n",
    "from src.rewards import FangerReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the EnergyPlus path to the system path\n",
    "sys.path.append('./EnergyPlus-23-1-0')\n",
    "# Set the EnergyPlus path as an environment variable\n",
    "os.environ['EPLUS_PATH'] = './EnergyPlus-23-1-0'\n",
    "\n",
    "# Ignore deprecation warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::DeprecationWarning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment ID\n",
    "id = \"Eplus-5zone-hot-continuous-stochastic-v1\"\n",
    "\n",
    "# Weather\n",
    "weather_files = ['USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw']\n",
    "\n",
    "# mu, sigma and theta for the weather variability\n",
    "# In the original version, weather_variability was a triple (mu, sigma, theta)\n",
    "# that only affected the drybulb (outdoor temperature), while now it is a dictionary\n",
    "# with the mean, sigma and theta for each weather variable we want to vary.\n",
    "# NOTE: Check apply_weather_variability method in CustomModelJSON class\n",
    "weather_variability = {\n",
    "    'drybulb': np.array([5.53173187e+00, 0.00000000e+00, 2.55034944e-03]), \n",
    "    'relhum': np.array([1.73128872e+01, 0.00000000e+00, 2.31712760e-03]), \n",
    "    'winddir': np.array([7.39984654e+01, 0.00000000e+00, 4.02298013e-04]), \n",
    "    'dirnorrad': np.array([3.39506556e+02, 0.00000000e+00, 9.78192172e-04]), \n",
    "    'windspd': np.array([1.64655725e+00, 0.00000000e+00, 3.45045547e-04])}\n",
    "\n",
    "variability_low = {\n",
    "    'drybulb': np.array([4.31066896e+00, 1.43882821e-03]), \n",
    "    'relhum': np.array([2.07871802e+01, 1.52442626e-03]), \n",
    "    'winddir': np.array([9.2461295e+01, 1.7792310e-04]), \n",
    "    'dirnorrad': np.array([2.26216882e+02, 3.96634341e-04]), \n",
    "    'windspd': np.array([1.92756975e+00, 2.60994514e-04])\n",
    "}\n",
    "\n",
    "variability_high = {\n",
    "    'drybulb': np.array([9.87995071e+00, 8.40623734e-03]), \n",
    "    'relhum': np.array([3.26129158e+01, 5.10374079e-03]), \n",
    "    'winddir': np.array([1.46046002e+02, 5.68863159e-04]), \n",
    "    'dirnorrad': np.array([3.51914077e+02, 8.28838542e-04]), \n",
    "    'windspd': np.array([3.73801488e+00, 8.64436358e-04])\n",
    "}\n",
    "\n",
    "# Custom reward derived from Fanger's comfort model.\n",
    "# This extends the LinearReward class from sinergym adding ppd and occupancy variables.\n",
    "reward = FangerReward\n",
    "reward_kwargs={\n",
    "    'temperature_variables': ['air_temperature'],\n",
    "    'energy_variables': ['HVAC_electricity_demand_rate'],\n",
    "    'range_comfort_winter': [20, 23],\n",
    "    'range_comfort_summer': [23, 26],\n",
    "    'energy_weight': 0.1,\n",
    "    'ppd_variable': 'Zone Thermal Comfort Fanger Model PPD(SPACE1-1 PEOPLE 1)',\n",
    "    'occupancy_variable': 'Zone People Occupant Count(SPACE1-1)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\n",
    "    id=id,\n",
    "    weather_files=weather_files,\n",
    "    reward=reward,\n",
    "    reward_kwargs=reward_kwargs,\n",
    "    weather_variability=weather_variability\n",
    ")\n",
    "\n",
    "# Wrap the environment (to account for the different weather variability)\n",
    "env =  SinergymWrapper(env)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    obs = env.reset()#, info,  = env.reset()\n",
    "    rewards = []\n",
    "    terminated = False\n",
    "    current_month = 0\n",
    "    while not terminated:\n",
    "        a = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from ray.rllib.models.catalog import MODEL_DEFAULTS\n",
    "from copy import deepcopy\n",
    "from src.utils import *\n",
    "\n",
    "from src.model import UncertainPPO\n",
    "\n",
    "def get_agent(\n",
    "        env, \n",
    "        callback_fn, \n",
    "        env_config, \n",
    "        eval_env_config, \n",
    "        args, \n",
    "        planning_model=None):\n",
    "    \n",
    "    # Initlaize the default configuration\n",
    "    config = UncertainPPO.get_default_config() \n",
    "    \n",
    "    # Configuration updates and customization\n",
    "    config[\"env\"] = env\n",
    "    config[\"seed\"] = 8765\n",
    "    config[\"ppo_framework\"] = \"torch\"\n",
    "    config[\"_disable_preprocessor_api\"] = True\n",
    "    config[\"rollout_fragment_length\"] = \"auto\"\n",
    "    config[\"env_config\"] = env_config\n",
    "    config[\"model\"] = MODEL_DEFAULTS\n",
    "    config[\"model\"] = {\n",
    "        \"fcnet_activation\": lambda: nn.Sequential(nn.Tanh(), nn.Dropout(p=0.1)),   # Custom_Activation\n",
    "        \"dropout\": 0.1,\n",
    "        \"num_dropout_evals\": 10,\n",
    "        \"max_seq_len\": 1,\n",
    "    }\n",
    "    config[\"train_batch_size\"] = 26280\n",
    "    config[\"num_sgd_iter\"] = 40\n",
    "    config[\"disable_env_checking\"] = True\n",
    "    config[\"clip_param\"] = 0.3\n",
    "    config[\"lr\"] = 5e-05\n",
    "    config[\"gamma\"] = 0.8\n",
    "    config[\"evaluation_interval\"] = 3\n",
    "    config[\"evaluation_duration\"] = 157680              # rrlib config\n",
    "    config[\"evaluation_duration_unit\"] = \"timesteps\"    #\"episodes\"\n",
    "    config[\"horizon\"] = 4380                            # rrlib config\n",
    "    config[\"soft_horizon\"] = True                       # rrlib config        \n",
    "    config[\"restart_failed_sub_environments\"] = True    # rrlib config\n",
    "    config[\"evaluation_sample_timeout_s\"] = 3600        # rrlib config\n",
    "    config[\"evaluation_parallel_to_training\"] = False   # rrlib config\n",
    "    config[\"evaluation_config\"] = {\n",
    "        \"env_config\": eval_env_config\n",
    "    }\n",
    "    config[\"callbacks\"] = lambda: callback_fn(\n",
    "        num_descent_steps=91,\n",
    "        batch_size=1, \n",
    "        no_coop=False, \n",
    "        planning_model=planning_model,\n",
    "        config=config, \n",
    "        run_active_rl=1.0, \n",
    "        planning_uncertainty_weight=1, \n",
    "        args=args, \n",
    "        uniform_reset=0.0)\n",
    "    \n",
    "    # Disable environment checking\n",
    "    config[\"disable_env_checking\"] = True\n",
    "    \n",
    "    # Initialize your custom or updated RLlib Algorithm\n",
    "    agent = UncertainPPO(config=config)\n",
    "\n",
    "    return agent\n",
    "\n",
    "def train_agent(agent, num_iterations=2):\n",
    "    # Simple training loop\n",
    "    for i in range(num_iterations):\n",
    "        result = agent.train()\n",
    "        print(f\"ITERATION: {i}, reward: {result['episode_reward_mean']}\")\n",
    "\n",
    "\n",
    "args = {\n",
    "    'num_gpus': 1, \n",
    "    'log_path': 'logs',\n",
    "    'project': 'active-rl', \n",
    "    'profile': False, \n",
    "    'env': 'sg', \n",
    "    'num_timesteps': 2, # 7500000, \n",
    "    'train_batch_size': 26280, \n",
    "    'horizon': 4380, \n",
    "    'clip_param': 0.3, \n",
    "    'lr': 5e-05, \n",
    "    'gamma': 0.8, \n",
    "    'num_sgd_iter': 40, \n",
    "    'eval_interval': 3, \n",
    "    'num_training_workers': 16, \n",
    "    'num_eval_workers': 16, \n",
    "    'num_envs_per_worker': 2, \n",
    "    #'cl_filename': './data/citylearn_challenge_2022_phase_1/schema.json', \n",
    "    #'cl_eval_folder': './data/all_buildings', \n",
    "    #'cl_use_rbc_residual': 0, \n",
    "    #'cl_action_multiplier': 1, \n",
    "    #'gw_filename': 'gridworlds/sample_grid.txt', \n",
    "    #'gw_steps_per_cell': 1, \n",
    "    #'dm_filename': 'gridworlds/sample_grid.txt', \n",
    "    'aliveness_reward': 0.01, \n",
    "    'distance_reward_scale': 0.01, \n",
    "    'use_all_geoms': False, \n",
    "    'walker': 'ant', \n",
    "    'dm_steps_per_cell': 1, \n",
    "    'control_timestep': 0.1, \n",
    "    'physics_timestep': 0.02, \n",
    "    'use_rbc': 0, \n",
    "    'use_random': 0, \n",
    "    'only_drybulb': False, \n",
    "    'sample_envs': False, \n",
    "    'sinergym_timesteps_per_hour': 1,\n",
    "    'eval_fidelity_ratio': 1, \n",
    "    'base_weather': 'hot', \n",
    "    'random_month': False, \n",
    "    'no_noise': False, \n",
    "    'continuous': True, \n",
    "    'only_vary_offset': True, \n",
    "    'use_activerl': 1.0, \n",
    "    'use_random_reset': 0.0, \n",
    "    'num_descent_steps': 91, \n",
    "    'no_coop': False, \n",
    "    'planning_model_ckpt': None, \n",
    "    'seed': 8765, \n",
    "    'planning_uncertainty_weight': 1, \n",
    "    'activerl_lr': 0.01, \n",
    "    'activerl_reg_coeff': 0.5, \n",
    "    'num_dropout_evals': 10, \n",
    "    'plr_d': 0.0, \n",
    "    'plr_beta': 0.1, \n",
    "    'plr_envs_to_1': 100, \n",
    "    'plr_robust': False, \n",
    "    'naive_grounding': False, \n",
    "    'env_repeat': 1, \n",
    "    'start': 0, \n",
    "    'plr_rho': 0.1, \n",
    "    'dropout': 0.1, \n",
    "    'full_eval_interval': 10, \n",
    "    'sinergym_sweep': '1.0,0,0,0'\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    'weather_variability': weather_variability, \n",
    "    'variability_low': variability_low, \n",
    "    'variability_high': variability_high, \n",
    "    'use_rbc': 0, \n",
    "    'use_random': 0, \n",
    "    'sample_environments': False, \n",
    "    'timesteps_per_hour': 1, \n",
    "    'weather_file': 'USA_AZ_Davis-Monthan.AFB.722745_TMY3.epw', \n",
    "    'epw_data': EPW_Data.load(\"data/US_epw_OU_data.pkl\"), \n",
    "    'continuous': True, \n",
    "    'random_month': False, \n",
    "    'only_vary_offset': True\n",
    "}\n",
    "\n",
    "eval_env_config = deepcopy(env_config)\n",
    "eval_env_config[\"act_repeat\"] = 1\n",
    "\n",
    "model_config = {}\n",
    "\n",
    "agent = get_agent(\n",
    "    SinergymWrapper,\n",
    "    SinergymCallback, \n",
    "    env_config, \n",
    "    eval_env_config, \n",
    "    model_config, \n",
    "    args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = agent.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_agent(agent, num_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
